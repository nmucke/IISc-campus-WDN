{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/scratch1/ntm/nikolaj_workstation_data/PhD/ML-for-WDN\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from ML_for_WDN.data_utils import clean_dataframes, load_data\n",
    "\n",
    "from ML_for_WDN.models import (\n",
    "    SupervisedReconstructionLeakDetector,\n",
    "    SupervisedLatentLeakDetector,\n",
    ")\n",
    "\n",
    "# set working directory to root directory\n",
    "%cd ..\n",
    "\n",
    "\n",
    "DATA_FILES = [\n",
    "    'data/data_no_leak.xlsx',\n",
    "    'data/data_leak_1.xlsx',\n",
    "    'data/data_leak_2.xlsx',\n",
    "    'data/data_leak_3.xlsx',\n",
    "]\n",
    "\n",
    "train_fraction = 0.8\n",
    "test_fraction = 0.2\n",
    "\n",
    "columns_to_use = [\n",
    "    'FM01_flow', 'FM02_head', 'FM03_flow', 'FM05_flow', 'FM06_flow', 'FM08_flow', 'FM09_flow', 'FM11_flow', 'FM13_flow',\n",
    "    'FM01_head', 'FM02_flow', 'FM03_head', 'FM05_head', 'FM06_head', 'FM08_head', 'FM09_head', 'FM11_head', 'FM13_head',\n",
    "]\n",
    "\n",
    "# Train data\n",
    "dataframes = []\n",
    "for data_file in DATA_FILES:\n",
    "    df = load_data(data_file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "dataframes = clean_dataframes(\n",
    "    dataframes,\n",
    "    columns_to_use=columns_to_use,\n",
    ")\n",
    "\n",
    "num_train_pr_dataframe = []\n",
    "num_test_pr_dataframe = []\n",
    "train_dataframes = []\n",
    "test_dataframes = []\n",
    "for df in dataframes:\n",
    "    num_train_pr_dataframe.append(int(train_fraction*df.shape[0]))\n",
    "    train_dataframes.append(df.iloc[:int(train_fraction*df.shape[0]), :])\n",
    "\n",
    "    num_test_pr_dataframe.append(int(test_fraction*df.shape[0]))\n",
    "    test_dataframes.append(df.iloc[-int(test_fraction*df.shape[0]):, :])\n",
    "\n",
    "X_train = pd.concat(train_dataframes, ignore_index=True).values\n",
    "X_test = pd.concat(test_dataframes, ignore_index=True).values\n",
    "\n",
    "# Create targets\n",
    "y_train = []\n",
    "y_test = []\n",
    "for i in range(len(num_train_pr_dataframe)):\n",
    "    y_train += [i]*num_train_pr_dataframe[i]\n",
    "    y_test += [i]*num_test_pr_dataframe[i]\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_args = {\n",
    "    'penalty': 'l2',\n",
    "    'C': 1e-2,\n",
    "    'solver': 'lbfgs',\n",
    "    'max_iter': 1000,\n",
    "}\n",
    "    \n",
    "svc_args = {\n",
    "    'C': 1e-2,\n",
    "    'kernel': 'rbf',\n",
    "    'gamma': 1e-2,\n",
    "}\n",
    "\n",
    "rf_args = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "}\n",
    "\n",
    "reconstruction_WAE_args = {\n",
    "    'encoder_args': {\n",
    "        'hidden_dims': [12, 10, 8],\n",
    "        'latent_dim': 4,\n",
    "    },\n",
    "    'decoder_args': {\n",
    "        'latent_dim': 4,\n",
    "        'hidden_dims': [8, 10, 12],\n",
    "        'num_pars': 4\n",
    "    },\n",
    "    'NN_train_args': {\n",
    "        'epochs': 5,\n",
    "        'batch_size': 2048,\n",
    "        'lr': 5e-3,\n",
    "        'weight_decay': 1e-10,\n",
    "        'loss_fn': nn.MSELoss(),\n",
    "    },\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "latent_WAE_args = {\n",
    "    'encoder_args': {\n",
    "        'hidden_dims': [12, 10, 8],\n",
    "        'latent_dim': 4,\n",
    "    },\n",
    "    'decoder_args': {\n",
    "        'latent_dim': 4,\n",
    "        'hidden_dims': [8, 10, 12],\n",
    "    },\n",
    "    'NN_train_args': {\n",
    "        'epochs': 5,\n",
    "        'batch_size': 2048,\n",
    "        'lr': 5e-3,\n",
    "        'weight_decay': 1e-10,\n",
    "        'loss_fn': nn.MSELoss(),\n",
    "    },\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Training stage 1 ##########\n",
      "\n",
      "\n",
      "Training autoencoder without leak data\n",
      "\n",
      "\n",
      "Autoencoder architecture:\n",
      "- Latent dimension: 4\n",
      "- Encoder hidden dimensions: [12, 10, 8]\n",
      "- Decoder hidden dimensions: [8, 10, 12]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5271 | Latent: 0.4733: 100%|██████████| 5/5 [00:37<00:00,  7.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Autoencoder training complete\n",
      "\n",
      "\n",
      "########## Training stage 2 ##########\n",
      "\n",
      "\n",
      "Training anomaly detector using autoencoder\n",
      "\n",
      "\n",
      "SupervisedLatentLeakDetector(NN_train_args={'batch_size': 2048, 'epochs': 5,\n",
      "                                            'loss_fn': MSELoss(), 'lr': 0.005,\n",
      "                                            'weight_decay': 1e-10},\n",
      "                             decoder_args={'hidden_dims': [8, 10, 12],\n",
      "                                           'latent_dim': 4, 'output_dim': 18},\n",
      "                             device='cpu',\n",
      "                             encoder_args={'hidden_dims': [12, 10, 8],\n",
      "                                           'input_dim': 18, 'latent_dim': 4}): Accuracy: 0.382\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    #LogisticRegression(**logistic_regression_args),\n",
    "    #SVC(**svc_args),\n",
    "    #RandomForestClassifier(**rf_args),\n",
    "    #SupervisedReconstructionLeakDetector(**reconstruction_WAE_args),\n",
    "    SupervisedLatentLeakDetector(**latent_WAE_args),\n",
    "]\n",
    "\n",
    "for model in model_list:\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler',  MinMaxScaler()),\n",
    "        ('model', model),\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "    )\n",
    "\n",
    "    preds = pipeline.predict(\n",
    "        X=X_test,\n",
    "    )\n",
    "    \n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print(f'{model}: Accuracy: {accuracy_score(y_test, preds):0.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
